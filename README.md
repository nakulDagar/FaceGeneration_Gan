# ðŸ§  GAN-Based Face Image Generator (128Ã—128 Resolution)

This project implements a **Generative Adversarial Network (GAN)** using TensorFlow and Keras to generate realistic human face images. The model progressively upsamples a noise vector into 128Ã—128 RGB face images. After **105 epochs** of training, it produces sharp, face-like structures with facial symmetry and texture.

<img width="721" height="662" alt="Screenshot 2025-07-27 at 11 32 15â€¯PM" src="https://github.com/user-attachments/assets/be7a1e4c-9507-470e-9747-cf841e8e41ad" />

---
```
# ðŸ“‚ Project Structure
â”œâ”€â”€ main.py                     # Load generator & generate sample images
â”œâ”€â”€ main.ipynb                   # Contains build_generator() and build_discriminator() and Custom training step function
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore                  # Ignore virtualenvs, checkpoints, caches
â””â”€â”€ README.md                   # Youâ€™re here!
```
---
## ðŸ“¦ Dependencies

Install dependencies from the `requirements.txt`:

```bash
pip install -r requirements.txt
```
Core Libraries Used:
	â€¢	tensorflow
	â€¢	numpy
	â€¢	matplotlib
	â€¢	tqdm
	â€¢	time, os (standard libraries)

# ðŸ§  Model Architecture

Generator (Upsampling: 100-dim â†’ 128Ã—128Ã—3)
- Dense(8Ã—8Ã—512) â†’ Reshape â†’ BatchNorm â†’ LeakyReLU
- Conv2DTranspose: 256 â†’ 128 â†’ 64 â†’ 32 â†’ 3
- Final activation: tanh (outputs in [-1, 1])

Discriminator (Downsampling: 128Ã—128Ã—3 â†’ binary output)
- Conv2D: 64 â†’ 128 â†’ 256 â†’ 512
- LeakyReLU + Dropout after each layer
- Flatten â†’ Dense(1) logit output

## ðŸ§ª Training Strategy

The models were trained adversarially using a custom training loop in TensorFlow:
```python
#Loss Functions:
#Binary crossentropy loss
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    return cross_entropy(tf.ones_like(real_output), real_output) + \
           cross_entropy(tf.zeros_like(fake_output), fake_output)

# Training Step:
def train_step(real_images):
    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])
    ...
    g_loss = generator_loss(fake_output)
    d_loss = discriminator_loss(real_output, fake_output)
    ...
    # Apply gradients to both networks
    
# Optimizers:

gen_optimizer = tf.keras.optimizers.Adam(1e-4)
disc_optimizer = tf.keras.optimizers.Adam(1e-4)
```
```txt
# ðŸ“ˆ Training Summary
Parameter                    Value

Dataset                      CelebA
Input Vector              100-dim noise
Image Size                 128Ã—128 RGB
Epochs                         105
Batch Size               128(customizable)
Loss Function           Binary Crossentropy
Optimizer                  Adam (1e-4)
Framework               TensorFlow 2.16.2
```
# ðŸ™Œ Acknowledgements
	â€¢	Ian Goodfellow et al., GAN Paper (2014)
	â€¢	TensorFlow & Keras Teams
	â€¢	CelebA Dataset


